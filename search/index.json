[{"content":"Who rebooted my VM? A cloud mystery I had been recently involved in an interesting discussion on how to figure out, who has rebooted your virtual machine from a linux perspective. In this particular case, we were talking about an OpenShift node. I will be elaborating on a few different cases and how those can be investigated.\nTo reproduce the problem, I am using a five node OpenShift cluster which is running on a Kernel Based Virtual Machine (KVM) host.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 $ oc get nodes NAME STATUS ROLES AGE VERSION compute-0 Ready worker 3d23h v1.31.6 compute-1 Ready worker 3d23h v1.31.6 master-0 Ready control-plane,master,worker 3d23h v1.31.6 master-1 Ready control-plane,master,worker 3d23h v1.31.6 master-2 Ready control-plane,master,worker 3d23h v1.31.6 $ virsh list Id Name State -------------------------------- 2 ocp4-master-0 running 3 ocp4-master-1 running 4 ocp4-master-2 running 5 ocp4-compute-0 running 6 ocp4-compute-1 running Consequently, I am my own hypervisor admin which will be sabotaging my OpenShift cluster for this investigation.\nWhat all cases will have in common is that a node will be NotReady for some time, e.g.:\n1 2 3 4 5 6 7 $ oc get nodes NAME STATUS ROLES AGE VERSION compute-0 Ready worker 4d15h v1.31.6 compute-1 NotReady worker 4d15h v1.31.6 master-0 Ready control-plane,master,worker 4d15h v1.31.6 master-1 Ready control-plane,master,worker 4d15h v1.31.6 master-2 Ready control-plane,master,worker 4d15h v1.31.6 Additionally you will be able to find logs of the machine config controller which reports a NotReady machine:\n1 2 3 4 5 6 7 8 $ oc -n openshift-machine-config-operator logs machine-config-controller-5f88fbbc4f-rqjm4 -c machine-config-controller | grep -i compute-1 ... I0404 06:41:13.153952 1 node_controller.go:584] Pool worker: node compute-1: Reporting unready: node compute-1 is reporting OutOfDisk=Unknown I0404 06:41:13.196590 1 node_controller.go:584] Pool worker: node compute-1: changed taints I0404 06:41:19.481728 1 node_controller.go:584] Pool worker: node compute-1: changed taints I0404 06:42:42.730148 1 node_controller.go:584] Pool worker: node compute-1: Reporting unready: node compute-1 is reporting NotReady=False ... I0404 06:42:56.309260 1 node_controller.go:584] Pool worker: node compute-1: Reporting ready Case 1: User reboot In this case a user has rebooted one of your nodes (either via a debug container or via ssh). This can be easily found with the audit logs:\n1 2 3 $ cat /var/log/audit/audit.log | grep reboot ... type=USER_CMD msg=audit(1743748835.979:805): pid=1366614 uid=0 auid=4294967295 ses=4294967295 subj=system_u:system_r:spc_t:s0 msg=\u0026#39;cwd=\u0026#34;/\u0026#34; cmd=\u0026#34;reboot\u0026#34; exe=\u0026#34;/usr/bin/sudo\u0026#34; terminal=? res=success\u0026#39;UID=\u0026#34;root\u0026#34; AUID=\u0026#34;unset\u0026#34; Case 2: Machine Config rollout A machine config rollout can also be found in the audit.log files:\n1 2 3 4 $ cat /var/log/audit/audit.log | grep reboot ... type=SERVICE_START msg=audit(1743789169.017:1277): pid=1 uid=0 auid=4294967295 ses=4294967295 subj=system_u:system_r:init_t:s0 msg=\u0026#39;unit=machine-config-daemon-reboot comm=\u0026#34;systemd\u0026#34; exe=\u0026#34;/usr/lib/systemd/systemd\u0026#34; hostname=? addr=? terminal=? res=success\u0026#39;^]UID=\u0026#34;root\u0026#34; AUID=\u0026#34;unset\u0026#34; type=SERVICE_STOP msg=audit(1743789169.119:1278): pid=1 uid=0 auid=4294967295 ses=4294967295 subj=system_u:system_r:init_t:s0 msg=\u0026#39;unit=machine-config-daemon-reboot comm=\u0026#34;systemd\u0026#34; exe=\u0026#34;/usr/lib/systemd/systemd\u0026#34; hostname=? addr=? terminal=? res=success\u0026#39;^]UID=\u0026#34;root\u0026#34; AUID=\u0026#34;unset Furthermore this can be confirmed with the MachineConfigPool, which provides us with the latest MachineConfig rendered-worker-04636f9acf052beffabf57725a564095\n1 2 3 $ oc get mcp worker NAME CONFIG UPDATED UPDATING DEGRADED MACHINECOUNT READYMACHINECOUNT UPDATEDMACHINECOUNT DEGRADEDMACHINECOUNT AGE worker rendered-worker-04636f9acf052beffabf57725a564095 True False False 2 2 2 0 5d2h Looking at the boot IDs, we should see, that boot id 57cdb3ac10fd46c3933a3f0ba3e3c33f should be shutdown by the machine config operator with a message, that it will boot into rendered-worker-04636f9acf052beffabf57725a564095. This is the case:\n1 2 3 $ journalctl --list-boots | tail -n 2 -1 57cdb3ac10fd46c3933a3f0ba3e3c33f Fri 2025-04-04 07:47:43 UTC Fri 2025-04-04 17:53:13 UTC 0 6004e8be8fbc4d769485a991ff1641d4 Fri 2025-04-04 17:53:19 UTC Fri 2025-04-04 18:04:01 UTC 1 2 3 4 5 6 $ journalctl -u machine-config-daemon-reboot ... -- Boot 57cdb3ac10fd46c3933a3f0ba3e3c33f -- Apr 04 17:52:49 compute-1 systemd[1]: Started machine-config-daemon: Node will reboot into config rendered-worker-04636f9acf052beffabf57725a564095. Apr 04 17:52:49 compute-1 systemd[1]: machine-config-daemon-reboot.service: Deactivated successfully. Apr 04 17:52:49 compute-1 systemd[1]: Stopped machine-config-daemon: Node will reboot into config rendered-worker-04636f9acf052beffabf57725a564095. Case 3: A container with elevated privileges was compromised Some containers require elevated privileges to perform certain actions on OpenShift. If such a container is compromised, then a reboot could be initiated from the container. In this example I will be using a container of the Node Tuning Operator:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ oc -n openshift-cluster-node-tuning-operator get pods -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES cluster-node-tuning-operator-5bb647f57c-lk7wx 1/1 Running 0 4d15h X.X.X.X master-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; tuned-5q7vm 1/1 Running 1 4d15h X.X.X.X compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; tuned-8n89v 1/1 Running 1 4d15h X.X.X.X compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; tuned-g7nwk 1/1 Running 0 4d15h X.X.X.X master-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; tuned-rxt2q 1/1 Running 0 4d15h X.X.X.X master-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; tuned-xsm98 1/1 Running 0 4d15h X.X.X.X master-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; $ oc -n openshift-cluster-node-tuning-operator rsh tuned-8n89v $ id uid=0(root) gid=0(root) groups=0(root) $ date Fri Apr 4 07:25:01 UTC 2025 $ reboot One interesting fact here is, that we do not see this reboot in the audit.log, as the last entry for a reboot is the same as the one from Case 1:\n1 2 $ cat /var/log/audit/audit.log | grep reboot | tail -n 1 type=USER_CMD msg=audit(1743748835.979:805): pid=1366614 uid=0 auid=4294967295 ses=4294967295 subj=system_u:system_r:spc_t:s0 msg=\u0026#39;cwd=\u0026#34;/\u0026#34; cmd=\u0026#34;reboot\u0026#34; exe=\u0026#34;/usr/bin/sudo\u0026#34; terminal=? res=success\u0026#39;UID=\u0026#34;root\u0026#34; AUID=\u0026#34;unset\u0026#34; Nevertheless, we can see in the journalctl that the reboot was initiated. Please also note, that the reboot was initiated shortly after Fri Apr 4 07:25:01 UTC 2025:\n1 2 3 4 $ journalctl -b -1 | grep -i reboot | tail -n 3 Apr 04 07:25:06 compute-1 systemd-logind[906]: The system will reboot now! Apr 04 07:25:06 compute-1 systemd-logind[906]: System is rebooting. Apr 04 07:26:41 compute-1 systemd[1]: Requested transaction contradicts existing jobs: Transaction for NetworkManager-dispatcher.service/start is destructive (reboot.target has \u0026#39;start\u0026#39; job queued, but \u0026#39;stop\u0026#39; is included in transaction). While an audit rule might be helping to detect that the reboot was started via /usr/sbin/reboot from a container, this is probably not something of practical relevance.\nCase 4: The hypervisor triggered a soft reboot To differentiate better between those cases, we will be using compute-0 for the reboots initiated by the hypervisor / cloud provider instead of compute-1. For a (kind of) graceful shutdown we will be using the virsh reboot command. To provide a rough idea what this does, we can take a look at the man page for virsh reboot:\nReboot a domain. This acts just as if the domain had the reboot command run from the console. The command returns as soon as it has executed the reboot action, which may be significantly before the domain actually reboots.\n1 2 3 4 5 $ date Fri Apr 4 06:59:52 PM CEST 2025 $ virsh reboot ocp4-compute-0 Domain \u0026#39;ocp4-compute-0\u0026#39; is being rebooted We can find a related audit.log:\n1 2 $ ausearch -m SERVICE_STOP | grep -Ei \u0026#39;shutdownd|shutdown|reboot|final|auditd\u0026#39; type=SERVICE_STOP msg=audit(1743785996.839:279): pid=1 uid=0 auid=4294967295 ses=4294967295 subj=system_u:system_r:init_t:s0 msg=\u0026#39;unit=dracut-shutdown comm=\u0026#34;systemd\u0026#34; exe=\u0026#34;/usr/lib/systemd/systemd\u0026#34; hostname=? addr=? terminal=? res=success\u0026#39; Additionally we see the shutdown process in the journalctl of the previous boot:\n1 2 3 4 5 6 7 $ journalctl -b -1 | grep -i shutdown | tail -n 6 Apr 04 16:59:56 compute-0 systemd[1]: Stopping Restore /run/initramfs on shutdown... Apr 04 16:59:56 compute-0 systemd[1]: dracut-shutdown.service: Deactivated successfully. Apr 04 16:59:56 compute-0 systemd[1]: Stopped Restore /run/initramfs on shutdown. Apr 04 17:00:42 compute-0 systemd[1]: Requested transaction contradicts existing jobs: Transaction for NetworkManager-dispatcher.service/start is destructive (shutdown.target has \u0026#39;start\u0026#39; job queued, but \u0026#39;stop\u0026#39; is included in transaction). Apr 04 17:00:43 compute-0 systemd[1]: Stopping Record System Boot/Shutdown in UTMP... Apr 04 17:00:43 compute-0 systemd[1]: Stopped Record System Boot/Shutdown in UTMP. Note: While the timezone configuration is different, those logs are related to the same virsh reboot.\nCase 5: The hypervisor triggered a hard reset For this experiment we will be using the virsh reset command. To provide a rough idea what this does, we can take a look at the man page for virsh reset:\nReset a domain immediately without any guest shutdown. reset emulates the power reset button on a machine, where all guest hardware sees the RST line set and reinitializes internal state. Note: Reset without any guest OS shutdown risks data loss.\n1 2 3 4 5 $ date Fri Apr 4 07:17:45 PM CEST 2025 $ virsh reset ocp4-compute-0 Domain \u0026#39;ocp4-compute-0\u0026#39; was reset The only audit.log we can find, is the one form before:\n1 2 $ ausearch -m SERVICE_STOP | grep -Ei \u0026#39;shutdownd|shutdown|reboot|final|auditd\u0026#39; type=SERVICE_STOP msg=audit(1743785996.839:279): pid=1 uid=0 auid=4294967295 ses=4294967295 subj=system_u:system_r:init_t:s0 msg=\u0026#39;unit=dracut-shutdown comm=\u0026#34;systemd\u0026#34; exe=\u0026#34;/usr/lib/systemd/systemd\u0026#34; hostname=? addr=? terminal=? res=success\u0026#39; The journalctl of the previous boot also suddenly ends without a shutdown sequence:\n1 2 3 $ journalctl -b -1 | tail -n 2 Apr 04 17:16:11 compute-0 systemd[1]: Finished Cleanup of Temporary Directories. Apr 04 17:16:11 compute-0 systemd[1]: run-credentials-systemd\\x2dtmpfiles\\x2dclean.service.mount: Deactivated successfully. Note: The shutdown was initiated at 17:17:45 when considering the different timezone configurations.\nNevertheless, we can have an indicator, that the previous boot had an unclean shutdown by seeing if there was\nA recovery started during latest boot A filesystem check initiated during latest boot. 1 2 3 4 5 6 7 8 $ journalctl -b 0 | grep -iE \u0026#39;recover|fsck\u0026#39; Apr 04 17:17:58 localhost systemd-fsck[651]: /usr/sbin/fsck.xfs: XFS file system. Apr 04 17:17:58 localhost kernel: XFS (vda4): Starting recovery (logdev: internal) \u0026lt;===== HERE Apr 04 17:17:58 localhost kernel: XFS (vda4): Ending recovery (logdev: internal) \u0026lt;===== HERE Apr 04 17:17:59 compute-0 systemd[1]: Created slice Slice /system/systemd-fsck. Apr 04 17:17:59 compute-0 systemd[1]: systemd-fsck-root.service: Deactivated successfully. Apr 04 17:18:00 compute-0 systemd-fsck[819]: boot: recovering journal \u0026lt;===== HERE Apr 04 17:18:00 compute-0 systemd-fsck[819]: boot: clean, 365/98304 files, 150426/393216 blocks We see that the system did not go through a normal shutdown sequence due to the presence of XFS recovery and fsck journal recovery which confirms that filesystems were not cleanly unmounted. See the unclean unmounts section in the RHEL 9 docs for XFS.\nCase 6: A kernel panic occured A kernel panic is a safety measure taken by the kernel of an operating system when an internal fatal error is detected, such as hardware failures and memory corruption. First we need to clarify, if a system would reboot in such a case. The responsible setting is kernel.panic, where 0 means that the system is not rebooted. Any other value indicates the number of seconds which are waited before the reboot is triggered, see further kernel docs.\n1 2 $ sysctl kernel.panic kernel.panic = 10 If a kernel panic would have happened, we would usually find a corresponding file in /sys/fs/pstore/\n1 2 $ ls /sys/fs/pstore/ sh-5.1# As the setup here is a bit more complicated for OpenShift, I will leave it without an example. For further readings, please follow the kernel docs.\nConclusion Linux offers many powerful utilities to understand what has happened on your system. This includes user initiated actions, hypervisor / cloud provider activity and even hardware failures. If you have other indicators which you are usually verifying, please let me know.\n","date":"2025-04-04T00:00:00Z","image":"https://www.niederwanger.dev/p/who-rebooted-my-vm/cover_hu_b4ce08ab093bd680.png","permalink":"https://www.niederwanger.dev/p/who-rebooted-my-vm/","title":"Who rebooted my VM? A cloud mystery"},{"content":"Bridging the Gap Between Virtual Machines and Kubernetes with KubeVirt This blog post will provide a brief introduction to the open source CNCF project KubeVirt. KubeVirt (alongside Kernel-based Virtual Machine (KVM)/libvirt/QEMU) is one of the foundational components included in OpenShift Virtualization. A more detailed relationship can be found on the Hyperconverged Cluster Operator for OpenShift Container Platform (OCP). While this blog post shall be focused on the upstream project KubeVirt, I have to provided full credit to OpenShift Virtualization along the lines, because most of my lessons learned are originated there.\nIf you are just interested in the demo, please head over to section KubeVirt with libvirt/QEMU/KVM in kind.\nWhy KubeVirt / OpenShift Virtualization? KubeVirt / OpenShift Virtualization allows you to operate virtual machines (VMs) on Kubernetes / OpenShift. I have come across several different reasons why companies would want to look into this topic. Please let me elaborate on some reasons:\nMigrate VMs to microservices at your pace KubeVirt / OpenShift Virtualization can be an interim solution when you are migrating your workloads towards containers anyway and need to host them on your k8s based platform temporary. This is especially true when you are using the Migration Toolkit for Virtualization on OpenShift for the initial migration. Legacy applications need VMs Some applications might not be worth to modernize to container technology as their is no commercial benefit. Thus you will leave them as is in a VM until the end of the application lifecycle. Reduce technology spread (Hypervisor + K8s / OCP) Some companies are looking for a way to reduce the technology spread in their existing infrastructure. Replace hypervisor and save costs $$$ The layer of a hypervisor is introducing additional costs on your infrastructure which can be avoided when using kubernetes / OpenShift on bare metal servers. Need virtualization technology for certain requirements Companies could be forced to use virtualization technology to comply with some of their requirements, especially in the realm of public and critical infrastructure. Why OpenShift Virtualization instead of upstream KubeVirt? From the above reasons you could think that OpenShift Virtualization might be fully interchangeable with KubeVirt. Nevertheless there are some key differences:\nOpenShift Virtualization is running on OpenShift thus uses Red Hat Enterprise Linux CoreOS (RHCOS) as the underlying operating system. RHCOS is the downstream version of Fedora CoreOS. Red Hat has a very long and successful history with Red Hat Enterprise Linux (RHEL) and Virtualization with the KVM/libvirt/QEMU stack in Red Hat Virtualization and Red Hat OpenStack Platform OpenShift has full commercial support from Red Hat. This includes multi-vendor support which can be relevant especially when you are facing issues on the storage side. OpenShift Virtualization can be easily setup and managed throughout the lifecycle via the Operator Lifecycle Manager on OpenShift. How does KubeVirt work? A detailed explanation about KubeVirt can be found on the official KubeVirt website. Simply put, KubeVirt will provide a Controller and API server on the control plane and some Agents on the worker nodes. You can interact with the custom resources via the kubernetes API server which will allow you to interact with the virtual machines. The virtual machines are created via the libvirt daemon which leverages QEMU and the KVM components on Linux.\nKubeVirt with libvirt/QEMU/KVM in kind Before we move ahead make sure to provision the setup on a linux machine, because there you will have access to the libvirt/QEMU/KVM stack. Other platform, e.g. MacOS, might use other hypervisor technology and you could run into issues. More specifically, you might run into kubevirt/issues/11917 on Apple hardware.\nSetup kind and KubeVirt To setup your machine please follow these instructions:\nSetup kubectl While I generally advocate for the rootless Podman, it is easier to get started with the Docker driver Setup kind Install virtctl a binary from the KubeVirt project to manage the VMs Via Github 1 2 3 4 5 6 export VERSION=$(curl -s https://storage.googleapis.com/kubevirt-prow/release/kubevirt/kubevirt/stable.txt) ARCH=$(uname -s | tr A-Z a-z)-$(uname -m | sed \u0026#39;s/x86_64/amd64/\u0026#39;) || windows-amd64.exe echo ${ARCH} curl -L -o virtctl https://github.com/kubevirt/kubevirt/releases/download/${VERSION}/virtctl-${VERSION}-${ARCH} chmod +x virtctl sudo install virtctl /usr/local/bin Via krew 1 kubectl krew install virt Install the libvirt/QEMU/KVM stack for your linux distribution. For Fedora you can find a guide here Provision kind cluster with KubeVirt First of all we create a cluster with one control-plane node and two worker nodes:\n1 2 3 4 5 6 7 8 cat \u0026lt;\u0026lt; EOF \u0026gt; kind-config.yaml kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane - role: worker - role: worker EOF Then we create a bash script which does the following:\nCreate a kind cluster with the kind-config.yaml config Following the kubevirt instructions to install the kubevirt operator Following the kubevirt lab to provision and start an ephemeral container disk VM Following the kubevirt instructions on Containerized Data Importer (CDI) operator to provision a persistent VM 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 #!/bin/bash set -euo pipefail # Enable strict error handling # Define color codes RED=\u0026#39;\\033[0;31m\u0026#39; # Red (Errors) GREEN=\u0026#39;\\033[0;32m\u0026#39; # Green (Success) YELLOW=\u0026#39;\\033[0;33m\u0026#39; # Yellow (Warnings) BLUE=\u0026#39;\\033[0;34m\u0026#39; # Blue (Info) NC=\u0026#39;\\033[0m\u0026#39; # No Color (Reset) # Function to check if a command exists command_exists() { command -v \u0026#34;$1\u0026#34; \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 } # Ensure required commands are installed for cmd in kind kubectl curl virtctl; do if ! command_exists \u0026#34;$cmd\u0026#34;; then echo -e \u0026#34;${RED}Error: \u0026#39;$cmd\u0026#39; is not installed. Please install it before running this script.${NC}\u0026#34; exit 1 fi done echo -e \u0026#34;${BLUE}Creating Kubernetes cluster...${NC}\u0026#34; kind create cluster --config=kind-config.yaml || { echo -e \u0026#34;${RED}Failed to create cluster${NC}\u0026#34;; exit 1; } echo -e \u0026#34;${BLUE}Fetching latest KubeVirt version...${NC}\u0026#34; VERSION=$(curl -fsSL https://storage.googleapis.com/kubevirt-prow/release/kubevirt/kubevirt/stable.txt) echo -e \u0026#34;${GREEN}Using KubeVirt version: $VERSION${NC}\u0026#34; echo -e \u0026#34;${BLUE}Deploying KubeVirt operator...${NC}\u0026#34; kubectl apply -f \u0026#34;https://github.com/kubevirt/kubevirt/releases/download/${VERSION}/kubevirt-operator.yaml\u0026#34; echo -e \u0026#34;${BLUE}Waiting for KubeVirt operator to be ready...${NC}\u0026#34; kubectl wait --for=condition=available --timeout=120s -n kubevirt deployments -l kubevirt.io echo -e \u0026#34;${BLUE}Deploying KubeVirt custom resource...${NC}\u0026#34; kubectl apply -f \u0026#34;https://github.com/kubevirt/kubevirt/releases/download/${VERSION}/kubevirt-cr.yaml\u0026#34; echo -e \u0026#34;${BLUE}Waiting for KubeVirt to be fully initialized...${NC}\u0026#34; kubectl wait --for=condition=Available --timeout=300s kubevirt/kubevirt -n kubevirt || { echo -e \u0026#34;${RED}KubeVirt deployment failed!${NC}\u0026#34; exit 1 } VM_MANIFEST=\u0026#34;https://kubevirt.io/labs/manifests/vm.yaml\u0026#34; echo -e \u0026#34;${BLUE}Applying Virtual Machine manifest from $VM_MANIFEST...${NC}\u0026#34; kubectl apply -f \u0026#34;$VM_MANIFEST\u0026#34; # Show VM YAML if \u0026#39;bat\u0026#39; is installed if command_exists bat; then echo -e \u0026#34;${BLUE}Displaying VM manifest...${NC}\u0026#34; curl -s \u0026#34;$VM_MANIFEST\u0026#34; | bat --language yaml fi # Wait briefly for the VM to be registered sleep 5 echo -e \u0026#34;${BLUE}Starting test VM...${NC}\u0026#34; virtctl start testvm || { echo -e \u0026#34;${RED}Failed to start VM${NC}\u0026#34;; exit 1; } echo -e \u0026#34;${GREEN} KubeVirt setup complete. Test VM started successfully!${NC}\u0026#34; # --------------------------------------------- # Containerized Data Importer (CDI) Deployment # --------------------------------------------- echo -e \u0026#34;${BLUE}Fetching latest Containerized Data Importer (CDI) version...${NC}\u0026#34; TAG=$(curl -s -w %{redirect_url} https://github.com/kubevirt/containerized-data-importer/releases/latest) VERSION=$(echo ${TAG##*/}) echo -e \u0026#34;${GREEN}Using CDI version: $VERSION${NC}\u0026#34; echo -e \u0026#34;${BLUE}Deploying CDI operator...${NC}\u0026#34; kubectl apply -f \u0026#34;https://github.com/kubevirt/containerized-data-importer/releases/download/$VERSION/cdi-operator.yaml\u0026#34; echo -e \u0026#34;${BLUE}Deploying CDI custom resource...${NC}\u0026#34; kubectl apply -f \u0026#34;https://github.com/kubevirt/containerized-data-importer/releases/download/$VERSION/cdi-cr.yaml\u0026#34; echo -e \u0026#34;${BLUE}Waiting for CDI to be fully initialized...${NC}\u0026#34; kubectl wait --for=condition=Available --timeout=300s cdi/cdi -n cdi || { echo -e \u0026#34;${RED}CDI deployment failed!${NC}\u0026#34; exit 1 } echo -e \u0026#34;${GREEN}CDI setup complete!${NC}\u0026#34; Experimenting with the containerdisk VM After executing the above bash script we should have the following nodes:\n1 2 3 4 5 $ kubectl get nodes AME STATUS ROLES AGE VERSION kind-control-plane Ready control-plane 22h v1.32.0 kind-worker Ready \u0026lt;none\u0026gt; 22h v1.32.0 kind-worker2 Ready \u0026lt;none\u0026gt; 22h v1.32.0 We have applied the following VirtualMachine to the cluster:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 apiVersion: kubevirt.io/v1 kind: VirtualMachine metadata: name: testvm spec: runStrategy: Halted template: metadata: labels: kubevirt.io/size: small kubevirt.io/domain: testvm spec: domain: devices: disks: - name: containerdisk disk: bus: virtio - name: cloudinitdisk disk: bus: virtio interfaces: - name: default masquerade: {} resources: requests: memory: 64M networks: - name: default pod: {} volumes: - name: containerdisk containerDisk: image: quay.io/kubevirt/cirros-container-disk-demo - name: cloudinitdisk cloudInitNoCloud: userDataBase64: SGkuXG4= This is a VirtualMachine which uses a containerDisk. A containerDisk is a ephemeral VM disk from a container image registry. We have not claimed any persistent storage with this VirtualMachine. Consequently will a reboot of this VM also result in a loss of all data.\n1 2 $ kubectl get pvc No resources found in default namespace Despite the fact, that the use of a containerDisk is not the main goal of KubeVirt, we can still use this VirtualMachine to understand the behaviour of KubeVirt components.\nThere should be a VirtualMachine in Running state:\n1 2 3 $ kubectl get virtualmachine NAME AGE STATUS READY testvm 22h Running True When a VirtualMachine is running, then there should also be a VirtualMachineInstance with the exact same name testvm:\n1 2 3 $ kubectl get virtualmachineinstance NAME AGE PHASE IP NODENAME READY testvm 20h Running 10.244.2.16 kind-worker True The VirtualMachine is actually running inside the virt-launcher-testvm-65f42 Pod:\n1 2 3 $ kubectl get pods NAME READY STATUS RESTARTS AGE virt-launcher-testvm-65f42 3/3 Running 0 20h We can now check which processes are running inside this virt-launcher-testvm-65f42 Pod:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 $ kubectl exec -it $(kubectl get pod -l=vm.kubevirt.io/name=testvm -o custom-columns=\u0026#39;:metadata.name\u0026#39; --no-headers) -- ps -ef UID PID PPID C STIME TTY TIME CMD qemu 1 0 0 Feb16 ? 00:00:00 /usr/bin/virt-launcher-monitor --qemu-timeout 312s --name testvm --uid a726afe8-d14f-4930-8240-86b47a25b547 --namespace default --kubevirt-share-dir /var/run/kubevirt --ephemeral-disk-dir /var/run/kubevirt-ephemeral-disks --container-disk-dir /var/run/kubevirt/container-disks --grace-period- seconds 45 --hook-sidecars 0 --ovmf-path /usr/share/OVMF --run-as-nonroot qemu 19 1 0 Feb16 ? 00:00:24 /usr/bin/virt-launcher --qemu-timeout 312s --name testvm --uid a726afe8-d14f-4930-8240-86b47a25b547 --namespace default --kubevirt-share-dir /var/run/kubevirt --ephemeral-disk-dir /var/run/kubevirt-ephemeral-disks --container-disk-dir /var/run/kubevirt/container-disks --grace-period-seconds 45 --hook-sidecars 0 --ovmf-path /usr/share/OVMF --run-as-nonroot qemu 31 19 0 Feb16 ? 00:00:02 /usr/sbin/virtqemud -f /var/run/libvirt/virtqemud.conf qemu 32 19 0 Feb16 ? 00:00:00 /usr/sbin/virtlogd -f /etc/libvirt/virtlogd.conf qemu 89 1 0 Feb16 ? 00:00:21 /usr/libexec/qemu-kvm -name guest=default_testvm,debug-threads=on -S -object {\u0026#34;qom-type\u0026#34;:\u0026#34;secret\u0026#34;,\u0026#34;id\u0026#34;:\u0026#34;masterKey0\u0026#34;,\u0026#34;for mat\u0026#34;:\u0026#34;raw\u0026#34;,\u0026#34;file\u0026#34;:\u0026#34;/var/run/kubevirt-private/libvirt/qemu/lib/domain-1-default_testvm/master-key.aes\u0026#34;} -machine pc-q35-rhel9.4.0,usb=off,dump-guest-core=off,memory-backend= pc.ram,acpi=on -accel kvm -cpu Haswell-noTSX-IBRS,vmx=on,pdcm=on,f16c=on,rdrand=on,hypervisor=on,vme=on,ss=on,arat=on,tsc-adjust=on,umip=on,md-clear=on,stibp=on,flush-l1d=o n,arch-capabilities=on,ssbd=on,xsaveopt=on,abm=on,pdpe1gb=on,ibpb=on,ibrs=on,amd-stibp=on,amd-ssbd=on,skip-l1dfl-vmentry=on,pschange-mc-no=on,gds-no=on,vmx-ins-outs=on,vmx- true-ctls=on,vmx-store-lma=on,vmx-activity-hlt=on,vmx-activity-wait-sipi=on,vmx-vmwrite-vmexit-fields=on,vmx-apicv-xapic=on,vmx-ept=on,vmx-desc-exit=on,vmx-rdtscp-exit=on,v mx-apicv-x2apic=on,vmx-vpid=on,vmx-wbinvd-exit=on,vmx-unrestricted-guest=on,vmx-apicv-register=on,vmx-apicv-vid=on,vmx-rdrand-exit=on,vmx-invpcid-exit=on,vmx-vmfunc=on,vmx- shadow-vmcs=on,vmx-pml=on,vmx-ept-execonly=on,vmx-page-walk-4=on,vmx-ept-2mb=on,vmx-ept-1gb=on,vmx-invept=on,vmx-eptad=on,vmx-invept-single-context=on,vmx-invept-all-contex t=on,vmx-invvpid=on,vmx-invvpid-single-addr=on,vmx-invvpid-all-context=on,vmx-intr-exit=on,vmx-nmi-exit=on,vmx-vnmi=on,vmx-preemption-timer=on,vmx-posted-intr=on,vmx-vintr- pending=on,vmx-tsc-offset=on,vmx-hlt-exit=on,vmx-invlpg-exit=on,vmx-mwait-exit=on,vmx-rdpmc-exit=on,vmx-rdtsc-exit=on,vmx-cr3-load-noexit=on,vmx-cr3-store-noexit=on,vmx-cr8 -load-exit=on,vmx-cr8-store-exit=on,vmx-flexpriority=on,vmx-vnmi-pending=on,vmx-movdr-exit=on,vmx-io-exit=on,vmx-io-bitmap=on,vmx-mtf=on,vmx-msr-bitmap=on,vmx-monitor-exit= on,vmx-pause-exit=on,vmx-secondary-ctls=on,vmx-exit-nosave-debugctl=on,vmx-exit-load-perf-global-ctrl=on,vmx-exit-ack-intr=on,vmx-exit-save-pat=on,vmx-exit-load-pat=on,vmx- exit-save-efer=on,vmx-exit-load-efer=on,vmx-exit-save-preemption-timer=on,vmx-entry-noload-debugctl=on,vmx-entry-ia32e-mode=on,vmx-entry-load-perf-global-ctrl=on,vmx-entry- load-pat=on,vmx-entry-load-efer=on,vmx-eptp-switching=on -m size=63488k -object {\u0026#34;qom-type\u0026#34;:\u0026#34;memory-backend-ram\u0026#34;,\u0026#34;id\u0026#34;:\u0026#34;pc.ram\u0026#34;,\u0026#34;size\u0026#34;:65011712} -overcommit mem-lock=off -sm p 1,sockets=1,dies=1,clusters=1,cores=1,threads=1 -object {\u0026#34;qom-type\u0026#34;:\u0026#34;iothread\u0026#34;,\u0026#34;id\u0026#34;:\u0026#34;iothread1\u0026#34;} -uuid 5a9fc181-957e-5c32-9e5a-2de5e9673531 -smbios type=1,manufacturer=Ku beVirt,product=None,uuid=5a9fc181-957e-5c32-9e5a-2de5e9673531,family=KubeVirt -no-user-config -nodefaults -chardev socket,id=charmonitor,fd=20,server=on,wait=off -mon chard ev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown -boot strict=on -device {\u0026#34;driver\u0026#34;:\u0026#34;pcie-root-port\u0026#34;,\u0026#34;port\u0026#34;:16,\u0026#34;chassis\u0026#34;:1,\u0026#34;id\u0026#34;:\u0026#34;pci.1\u0026#34;,\u0026#34;bus\u0026#34;:\u0026#34;pcie.0\u0026#34;,\u0026#34;mult ifunction\u0026#34;:true,\u0026#34;addr\u0026#34;:\u0026#34;0x2\u0026#34;} -device {\u0026#34;driver\u0026#34;:\u0026#34;pcie-root-port\u0026#34;,\u0026#34;port\u0026#34;:17,\u0026#34;chassis\u0026#34;:2,\u0026#34;id\u0026#34;:\u0026#34;pci.2\u0026#34;,\u0026#34;bus\u0026#34;:\u0026#34;pcie.0\u0026#34;,\u0026#34;addr\u0026#34;:\u0026#34;0x2.0x1\u0026#34;} -device {\u0026#34;driver\u0026#34;:\u0026#34;pcie-root-port\u0026#34;,\u0026#34;por t\u0026#34;:18,\u0026#34;chassis\u0026#34;:3,\u0026#34;id\u0026#34;:\u0026#34;pci.3\u0026#34;,\u0026#34;bus\u0026#34;:\u0026#34;pcie.0\u0026#34;,\u0026#34;addr\u0026#34;:\u0026#34;0x2.0x2\u0026#34;} -device {\u0026#34;driver\u0026#34;:\u0026#34;pcie-root-port\u0026#34;,\u0026#34;port\u0026#34;:19,\u0026#34;chassis\u0026#34;:4,\u0026#34;id\u0026#34;:\u0026#34;pci.4\u0026#34;,\u0026#34;bus\u0026#34;:\u0026#34;pcie.0\u0026#34;,\u0026#34;addr\u0026#34;:\u0026#34;0x2.0x3\u0026#34;} -devi ce {\u0026#34;driver\u0026#34;:\u0026#34;pcie-root-port\u0026#34;,\u0026#34;port\u0026#34;:20,\u0026#34;chassis\u0026#34;:5,\u0026#34;id\u0026#34;:\u0026#34;pci.5\u0026#34;,\u0026#34;bus\u0026#34;:\u0026#34;pcie.0\u0026#34;,\u0026#34;addr\u0026#34;:\u0026#34;0x2.0x4\u0026#34;} -device {\u0026#34;driver\u0026#34;:\u0026#34;pcie-root-port\u0026#34;,\u0026#34;port\u0026#34;:21,\u0026#34;chassis\u0026#34;:6,\u0026#34;id\u0026#34;:\u0026#34;pci.6\u0026#34;,\u0026#34;bus \u0026#34;:\u0026#34;pcie.0\u0026#34;,\u0026#34;addr\u0026#34;:\u0026#34;0x2.0x5\u0026#34;} -device {\u0026#34;driver\u0026#34;:\u0026#34;pcie-root-port\u0026#34;,\u0026#34;port\u0026#34;:22,\u0026#34;chassis\u0026#34;:7,\u0026#34;id\u0026#34;:\u0026#34;pci.7\u0026#34;,\u0026#34;bus\u0026#34;:\u0026#34;pcie.0\u0026#34;,\u0026#34;addr\u0026#34;:\u0026#34;0x2.0x6\u0026#34;} -device {\u0026#34;driver\u0026#34;:\u0026#34;pcie-root-port\u0026#34;,\u0026#34;port \u0026#34;:23,\u0026#34;chassis\u0026#34;:8,\u0026#34;id\u0026#34;:\u0026#34;pci.8\u0026#34;,\u0026#34;bus\u0026#34;:\u0026#34;pcie.0\u0026#34;,\u0026#34;addr\u0026#34;:\u0026#34;0x2.0x7\u0026#34;} -device {\u0026#34;driver\u0026#34;:\u0026#34;pcie-root-port\u0026#34;,\u0026#34;port\u0026#34;:24,\u0026#34;chassis\u0026#34;:9,\u0026#34;id\u0026#34;:\u0026#34;pci.9\u0026#34;,\u0026#34;bus\u0026#34;:\u0026#34;pcie.0\u0026#34;,\u0026#34;multifunction\u0026#34;:true,\u0026#34;ad dr\u0026#34;:\u0026#34;0x3\u0026#34;} -device {\u0026#34;driver\u0026#34;:\u0026#34;pcie-root-port\u0026#34;,\u0026#34;port\u0026#34;:25,\u0026#34;chassis\u0026#34;:10,\u0026#34;id\u0026#34;:\u0026#34;pci.10\u0026#34;,\u0026#34;bus\u0026#34;:\u0026#34;pcie.0\u0026#34;,\u0026#34;addr\u0026#34;:\u0026#34;0x3.0x1\u0026#34;} -device {\u0026#34;driver\u0026#34;:\u0026#34;virtio-scsi-pci-non-transitional\u0026#34;,\u0026#34;id \u0026#34;:\u0026#34;scsi0\u0026#34;,\u0026#34;bus\u0026#34;:\u0026#34;pci.5\u0026#34;,\u0026#34;addr\u0026#34;:\u0026#34;0x0\u0026#34;} -device {\u0026#34;driver\u0026#34;:\u0026#34;virtio-serial-pci-non-transitional\u0026#34;,\u0026#34;id\u0026#34;:\u0026#34;virtio-serial0\u0026#34;,\u0026#34;bus\u0026#34;:\u0026#34;pci.6\u0026#34;,\u0026#34;addr\u0026#34;:\u0026#34;0x0\u0026#34;} -blockdev {\u0026#34;driver\u0026#34;:\u0026#34;file\u0026#34;,\u0026#34;f ilename\u0026#34;:\u0026#34;/var/run/kubevirt/container-disks/disk_0.img\u0026#34;,\u0026#34;node-name\u0026#34;:\u0026#34;libvirt-3-storage\u0026#34;,\u0026#34;auto-read-only\u0026#34;:true,\u0026#34;discard\u0026#34;:\u0026#34;unmap\u0026#34;,\u0026#34;cache\u0026#34;:{\u0026#34;direct\u0026#34;:true,\u0026#34;no-flush\u0026#34;:false}} -b lockdev {\u0026#34;node-name\u0026#34;:\u0026#34;libvirt-3-format\u0026#34;,\u0026#34;read-only\u0026#34;:true,\u0026#34;discard\u0026#34;:\u0026#34;unmap\u0026#34;,\u0026#34;cache\u0026#34;:{\u0026#34;direct\u0026#34;:true,\u0026#34;no-flush\u0026#34;:false},\u0026#34;driver\u0026#34;:\u0026#34;qcow2\u0026#34;,\u0026#34;file\u0026#34;:\u0026#34;libvirt-3-storage\u0026#34;} -blockdev { \u0026#34;driver\u0026#34;:\u0026#34;file\u0026#34;,\u0026#34;filename\u0026#34;:\u0026#34;/var/run/kubevirt-ephemeral-disks/disk-data/containerdisk/disk.qcow2\u0026#34;,\u0026#34;node-name\u0026#34;:\u0026#34;libvirt-2-storage\u0026#34;,\u0026#34;auto-read-only\u0026#34;:true,\u0026#34;discard\u0026#34;:\u0026#34;unmap\u0026#34;,\u0026#34;c ache\u0026#34;:{\u0026#34;direct\u0026#34;:true,\u0026#34;no-flush\u0026#34;:false}} -blockdev {\u0026#34;node-name\u0026#34;:\u0026#34;libvirt-2-format\u0026#34;,\u0026#34;read-only\u0026#34;:false,\u0026#34;discard\u0026#34;:\u0026#34;unmap\u0026#34;,\u0026#34;cache\u0026#34;:{\u0026#34;direct\u0026#34;:true,\u0026#34;no-flush\u0026#34;:false},\u0026#34;driver\u0026#34;:\u0026#34;qco w2\u0026#34;,\u0026#34;file\u0026#34;:\u0026#34;libvirt-2-storage\u0026#34;,\u0026#34;backing\u0026#34;:\u0026#34;libvirt-3-format\u0026#34;} -device {\u0026#34;driver\u0026#34;:\u0026#34;virtio-blk-pci-non-transitional\u0026#34;,\u0026#34;bus\u0026#34;:\u0026#34;pci.7\u0026#34;,\u0026#34;addr\u0026#34;:\u0026#34;0x0\u0026#34;,\u0026#34;drive\u0026#34;:\u0026#34;libvirt-2-format\u0026#34;,\u0026#34;id\u0026#34;: \u0026#34;ua-containerdisk\u0026#34;,\u0026#34;bootindex\u0026#34;:1,\u0026#34;write-cache\u0026#34;:\u0026#34;on\u0026#34;,\u0026#34;werror\u0026#34;:\u0026#34;stop\u0026#34;,\u0026#34;rerror\u0026#34;:\u0026#34;stop\u0026#34;} -blockdev {\u0026#34;driver\u0026#34;:\u0026#34;file\u0026#34;,\u0026#34;filename\u0026#34;:\u0026#34;/var/run/kubevirt-ephemeral-disks/cloud-init-dat a/default/testvm/noCloud.iso\u0026#34;,\u0026#34;node-name\u0026#34;:\u0026#34;libvirt-1-storage\u0026#34;,\u0026#34;read-only\u0026#34;:false,\u0026#34;discard\u0026#34;:\u0026#34;unmap\u0026#34;,\u0026#34;cache\u0026#34;:{\u0026#34;direct\u0026#34;:true,\u0026#34;no-flush\u0026#34;:false}} -device {\u0026#34;driver\u0026#34;:\u0026#34;virtio-blk-pc i-non-transitional\u0026#34;,\u0026#34;bus\u0026#34;:\u0026#34;pci.8\u0026#34;,\u0026#34;addr\u0026#34;:\u0026#34;0x0\u0026#34;,\u0026#34;drive\u0026#34;:\u0026#34;libvirt-1-storage\u0026#34;,\u0026#34;id\u0026#34;:\u0026#34;ua-cloudinitdisk\u0026#34;,\u0026#34;write-cache\u0026#34;:\u0026#34;on\u0026#34;,\u0026#34;werror\u0026#34;:\u0026#34;stop\u0026#34;,\u0026#34;rerror\u0026#34;:\u0026#34;stop\u0026#34;} -netdev {\u0026#34;type\u0026#34;:\u0026#34;tap\u0026#34; ,\u0026#34;fd\u0026#34;:\u0026#34;21\u0026#34;,\u0026#34;vhost\u0026#34;:true,\u0026#34;vhostfd\u0026#34;:\u0026#34;23\u0026#34;,\u0026#34;id\u0026#34;:\u0026#34;hostua-default\u0026#34;} -device {\u0026#34;driver\u0026#34;:\u0026#34;virtio-net-pci-non-transitional\u0026#34;,\u0026#34;host_mtu\u0026#34;:1500,\u0026#34;netdev\u0026#34;:\u0026#34;hostua-default\u0026#34;,\u0026#34;id\u0026#34;:\u0026#34;ua-default \u0026#34;,\u0026#34;mac\u0026#34;:\u0026#34;f6:45:e5:7e:ae:be\u0026#34;,\u0026#34;bus\u0026#34;:\u0026#34;pci.1\u0026#34;,\u0026#34;addr\u0026#34;:\u0026#34;0x0\u0026#34;,\u0026#34;romfile\u0026#34;:\u0026#34;\u0026#34;} -add-fd set=0,fd=19,opaque=serial0-log -chardev socket,id=charserial0,fd=17,server=on,wait=off,logfile= /dev/fdset/0,logappend=on -device {\u0026#34;driver\u0026#34;:\u0026#34;isa-serial\u0026#34;,\u0026#34;chardev\u0026#34;:\u0026#34;charserial0\u0026#34;,\u0026#34;id\u0026#34;:\u0026#34;serial0\u0026#34;,\u0026#34;index\u0026#34;:0} -chardev socket,id=charchannel0,fd=18,server=on,wait=off -device {\u0026#34;driver\u0026#34;:\u0026#34;virtserialport\u0026#34;,\u0026#34;bus\u0026#34;:\u0026#34;virtio-serial0.0\u0026#34;,\u0026#34;nr\u0026#34;:1,\u0026#34;chardev\u0026#34;:\u0026#34;charchannel0\u0026#34;,\u0026#34;id\u0026#34;:\u0026#34;channel0\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;org.qemu.guest_agent.0\u0026#34;} -audiodev {\u0026#34;id\u0026#34;:\u0026#34;audio1\u0026#34;,\u0026#34;driver\u0026#34;:\u0026#34;none \u0026#34;} -vnc vnc=unix:/var/run/kubevirt-private/a726afe8-d14f-4930-8240-86b47a25b547/virt-vnc,audiodev=audio1 -device {\u0026#34;driver\u0026#34;:\u0026#34;VGA\u0026#34;,\u0026#34;id\u0026#34;:\u0026#34;video0\u0026#34;,\u0026#34;vgamem_mb\u0026#34;:16,\u0026#34;bus\u0026#34;:\u0026#34;pcie.0\u0026#34; ,\u0026#34;addr\u0026#34;:\u0026#34;0x1\u0026#34;} -global ICH9-LPC.noreboot=off -watchdog-action reset -device {\u0026#34;driver\u0026#34;:\u0026#34;virtio-balloon-pci-non-transitional\u0026#34;,\u0026#34;id\u0026#34;:\u0026#34;balloon0\u0026#34;,\u0026#34;free-page-reporting\u0026#34;:true,\u0026#34;bus\u0026#34; :\u0026#34;pci.9\u0026#34;,\u0026#34;addr\u0026#34;:\u0026#34;0x0\u0026#34;} -sandbox on,obsolete=deny,elevateprivileges=deny,spawn=deny,resourcecontrol=deny -msg timestamp=on qemu 156 0 0 14:06 pts/0 00:00:00 ps -ef Note: To get the full output of ps -ef it is easier to kubectl exec -it ... -- /bin/bash and then obtain the output with ps -ef | more.\nLet us elaborate on the above ouput in more detail.\nWe can see that the container is started with PID 1 from /usr/bin/virt-launcher-monitor which creates another process via /usr/bin/virt-launcher. The virt-launcher created another process via the libvirt QEMU management daemon /usr/sbin/virtqemud. What we can also clearly see is why the libvirt project exists as an abstraction layer. Starting the qemu-kvm with PID 89 with all of the parameters would be a very tough challenge without the libvirt layer in between.\nIf we want to look at the process tree more compactly, we can use pstree:\n1 2 3 4 5 6 kubectl exec -it $(kubectl get po -l=vm.kubevirt.io/name=testvm -o custom-columns=\u0026#39;:metadata.name\u0026#39; --no-headers) -- pstree virt-launcher-m-+-qemu-kvm---5*[{qemu-kvm}] |-virt-launcher-+-virtlogd---{virtlogd} | |-virtqemud---18*[{virtqemud}] | `-19*[{virt-launcher}] `-7*[{virt-launcher-m}] We can also leverage the libvirt project so see and understand the running VM.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 $ kubectl exec -it $(kubectl get pod -l=vm.kubevirt.io/name=testvm -o custom-columns=\u0026#39;:metadata.name\u0026#39; --no-headers) -- virsh list Authorization not available. Check if polkit service is running or see debug message for more information. Id Name State -------------------------------- 1 default_testvm running $ kubectl exec -it $(kubectl get pod -l=vm.kubevirt.io/name=testvm -o custom-columns=\u0026#39;:metadata.name\u0026#39; --no-headers) -- virsh dominfo default_testvm Authorization not available. Check if polkit service is running or see debug message for more information. Id: 1 Name: default_testvm UUID: 5a9fc181-957e-5c32-9e5a-2de5e9673531 OS Type: hvm State: running CPU(s): 1 CPU time: 23.1s Max memory: 63488 KiB Used memory: 62500 KiB Persistent: yes Autostart: disable Managed save: no Security model: none Security DOI: 0 In theory you could also manipulate the VM state with the virsh commands, but this is not the real intention of the KubeVirt project. Nevertheless, we will do that for the sake of our understanding. With virsh destroy \u0026lt;DOMAIN\u0026gt; we can forcefully stop a VM:\n1 2 3 4 5 6 7 8 9 10 11 12 $ kubectl exec -it $(kubectl get pod -l=vm.kubevirt.io/name=testvm -o custom-columns=\u0026#39;:metadata.name\u0026#39; --no-headers) -- virsh destroy default_testvm Authorization not available. Check if polkit service is running or see debug message for more information. Domain \u0026#39;default_testvm\u0026#39; destroyed $ kubectl get pods -w NAME READY STATUS RESTARTS AGE virt-launcher-testvm-v5h4f 0/3 Init:0/2 0 0s virt-launcher-testvm-v5h4f 0/3 Init:0/2 0 1s virt-launcher-testvm-v5h4f 0/3 Init:1/2 0 2s virt-launcher-testvm-v5h4f 0/3 PodInitializing 0 9s virt-launcher-testvm-v5h4f 3/3 Running 0 20s virt-launcher-testvm-v5h4f 3/3 Running 0 20s The VirtualMachine will automatically be restarted because we have set the spec.runStrategy: Always with our startup script.\n1 2 $ kubectl get vm testvm -oyaml | yq -r \u0026#39;.spec.runStrategy\u0026#39; Always We can also access the VirtualMachine with the virtctl console command. Let\u0026rsquo;s create a file inside the VM:\n1 2 3 4 5 6 7 8 9 10 11 $ virtctl console testvm Successfully connected to testvm console. The escape sequence is ^] login as \u0026#39;cirros\u0026#39; user. default password: \u0026#39;gocubsgo\u0026#39;. use \u0026#39;sudo\u0026#39; for root. testvm login: cirros Password: $ touch test.txt $ uptime 15:46:49 up 28 min, 1 users, load average: 0.00, 0.00, 0.00 $ ls test.txt Now let us migrate the VM to another node and observe the virtualmachineinstancemigrations:\n1 2 3 4 5 6 7 8 9 10 11 12 13 $ virtctl migrate testvm VM testvm was scheduled to migrate $ kubectl get virtualmachineinstancemigrations -w NAME PHASE VMI kubevirt-migrate-vm-kbtgn Scheduling testvm kubevirt-migrate-vm-kbtgn Scheduled testvm kubevirt-migrate-vm-kbtgn PreparingTarget testvm kubevirt-migrate-vm-kbtgn TargetReady testvm kubevirt-migrate-vm-kbtgn Running testvm kubevirt-migrate-vm-kbtgn Succeeded testvm kubevirt-migrate-vm-kbtgn Succeeded testvm kubevirt-migrate-vm-kbtgn Succeeded testvm The migration is completed, which we can also observe on the Pods, as the virt-launcher-testvm-v5h4f on node kind-worker is gone and spawned another one on kind-worker2. The VirtualMachine is still running:\n1 2 3 4 5 6 7 8 9 10 11 12 $ kubectl get pods -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES virt-launcher-testvm-9ggsf 3/3 Running 0 41s 10.244.1.8 kind-worker2 \u0026lt;none\u0026gt; 1/1 virt-launcher-testvm-v5h4f 0/3 Completed 0 29m 10.244.2.20 kind-worker \u0026lt;none\u0026gt; 1/1 $ virtctl console testvm Successfully connected to testvm console. The escape sequence is ^] $ ls test.txt $ uptime 15:48:10 up 29 min, 1 users, load average: 0.00, 0.00, 0.00 As pointed out earlier, a restart of the VM will result in a loss of all data due to the containerDisk:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 $ virtctl restart testvm VM testvm was scheduled to restart $ kubectl get pods -w NAME READY STATUS RESTARTS AGE virt-launcher-testvm-9ggsf 3/3 Terminating 0 6m28s virt-launcher-testvm-9ggsf 0/3 Completed 0 6m29s virt-launcher-testvm-4sn54 0/3 Pending 0 0s virt-launcher-testvm-4sn54 0/3 Init:0/2 0 0s virt-launcher-testvm-4sn54 0/3 Init:1/2 0 2s virt-launcher-testvm-4sn54 0/3 PodInitializing 0 8s virt-launcher-testvm-4sn54 3/3 Running 0 21s $ virtctl console testvmvm Successfully connected to testvm console. The escape sequence is ^] login as \u0026#39;cirros\u0026#39; user. default password: \u0026#39;gocubsgo\u0026#39;. use \u0026#39;sudo\u0026#39; for root. testvm login: cirros Password: $ ls $ uptime 15:54:16 up 0 min, 1 users, load average: 0.00, 0.00, 0.00 Experimenting with a persistent VM Now let us take a look at a VirtualMachine which is using a Read Write Once (RWO) persistent storage and a fedora 41 cloud image. This VirtualMachine is relying on the Containerized Data Importer (CDI) installed with the previous bash script. The CDI will import the cloud image into a DataVolume which is an abstraction on top of a Persistent Volume Claim (PVC).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 apiVersion: kubevirt.io/v1 kind: VirtualMachine metadata: labels: kubevirt.io/vm: fedora-vm name: fedora-vm spec: dataVolumeTemplates: - metadata: creationTimestamp: null name: src-dv spec: storage: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 10Gi storageClassName: standard source: http: url: https://download.fedoraproject.org/pub/fedora/linux/releases/41/Cloud/x86_64/images/Fedora-Cloud-Base-Generic-41-1.4.x86_64.qcow2 runStrategy: Always template: metadata: labels: kubevirt.io/vm: fedora-vm spec: domain: cpu: sockets: 1 cores: 1 threads: 1 devices: disks: - disk: bus: virtio name: datavolumedisk1 interfaces: - masquerade: {} name: default resources: requests: memory: 2Gi terminationGracePeriodSeconds: 0 volumes: - dataVolume: name: src-dv name: datavolumedisk1 - cloudInitNoCloud: userData: |- #cloud-config user: fedora password: fedora chpasswd: { expire: False } runcmd: - sudo dnf update - sudo dnf install httpd -y - sudo sed -iE \u0026#39;s/Listen 80/Listen 8080/\u0026#39; /etc/httpd/conf/httpd.conf - sudo systemctl enable httpd.service - sudo systemctl start httpd.service - echo \u0026#39;This is demo VM 1 :)\u0026#39; \u0026gt; /var/www/html/index.html name: cloudinitdisk networks: - name: default pod: {} This VM has dedicated cpu resources in .spec.template.spec.domain.cpu which are actually the default values. Additionally we are installing httpd, operate it on port 8080 and write our custom index.html file via the cloud-init.\nLet\u0026rsquo;s create the VM and expose a NodePort service:\n1 2 3 4 5 $ kubectl apply -f fedora-vm.yaml $ virtctl expose vmi fedora-vm --name=fedora-http --port=8080 --type=NodePort $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE fedora-http NodePort 10.96.225.134 \u0026lt;none\u0026gt; 8080:31538/TCP 22h We can also reach it with curl:\n1 2 $ curl $(kubectl get node kind-worker -ojson | jq -r \u0026#39;.status.addresses[0].address\u0026#39;):$(kubectl get svc fedora-http -ojson | jq -r \u0026#39;.spec.ports[0].nodePort\u0026#39;) This is demo VM 1 :) This VirtualMachine is now utilizing a persistent storage via DataVolume and PVC:\n1 2 3 4 5 6 7 $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS VOLUMEATTRIBUTESCLASS AGE src-dv Bound pvc-66eec301-9ec0-48c9-988f-f3383dbdf2ec 11362347344 RWO standard \u0026lt;unset\u0026gt; 2m37s $ kubectl get dv NAME PHASE PROGRESS RESTARTS AGE src-dv Succeeded 100.0% 2m41s Note: The PVC is RWO, which does not allow us to live migrate the VM between hosts. Thus we had explored the live migration with the containerDisk VM in the previous section.\nWe will test the persistency of the storage:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 $ virtctl console fedora-vm Successfully connected to fedora-vm console. The escape sequence is ^] fedora-vm login: fedora Password: [fedora@fedora-vm ~]$ ls [fedora@fedora-vm ~]$ touch test.txt [fedora@fedora-vm ~]$ exit $ virtctl restart fedora-vm VM fedora-vm was scheduled to restart $ kubectl get pods -w NAME READY STATUS RESTARTS AGE virt-launcher-fedora-vm-4gtvx 0/2 ContainerCreating 0 2s virt-launcher-testvm-4sn54 3/3 Running 0 27m virt-launcher-fedora-vm-4gtvx 2/2 Running 0 4s $ virtctl console fedora-vm Successfully connected to fedora-vm console. The escape sequence is ^] fedora-vm login: fedora Password: Last login: Mon Feb 17 16:22:18 on ttyS0 [fedora@fedora-vm ~]$ ls test.txt ","date":"2025-02-18T00:00:00Z","image":"https://www.niederwanger.dev/p/kubevirt-to-run-vms-on-k8s/cover_hu_5abfb00ffe08a534.png","permalink":"https://www.niederwanger.dev/p/kubevirt-to-run-vms-on-k8s/","title":"KubeVirt with libvirt/QEMU/KVM in Kind"},{"content":"A retrospective Ten years ago I have used my exact same workstation hardware to try my first linux desktop experience. My hardware is:\nIntel Xeon CPU E3-1231 v3 Nvidia GTX 970 broke twice, but one time within warranty replaced it with a GTX 1050Ti 16 GB DDR3 RAM As you can tell, that is not really a powerful hardware nowadays anymore.\nBack then I was using Ubuntu in dual boot with Windows 7. As I was not sure, if Linux would actually work as I hoped it to work, I didn\u0026rsquo;t dare to completely wipe my Windows. Fortunately, I haven\u0026rsquo;t done it as my Linux desktop experience was quite tedious. The biggest pain points I was facing were:\nI was not that familiar with Linux itself (at least compared to now) Installing the correct hardware drivers was a nightmare Most games didn\u0026rsquo;t offer native Linux support thus you had to rely heavily on wine Almost no native gaming clients I could not get some games working on linux Looking at forums you occasionally found threads about people complaining to be banned, because they were playing on a non supported operating system Why now? As time has passed, I was evaluating if I want to migrate my Windows 10 to Windows 11. After checking the system requirements for Windows 11, I found out that my PC was lacking on the following parts:\nConvert Master Boot Record (MBR) to GUID Partition Table (GPT) to enable UEFI boot No Trusted Platform Module (TPM) Unsupported processor While converting the MBR to GPT was a software issue and can be solved with this guide, the remaining items are hardware issues. There might be the possibility to work around this, but I didn\u0026rsquo;t like the direction Windows was aiming for anyway. Consequently, I have decided to give Linux another chance.\nThe Linux Experience Which Distro? One of the first questions which come up, when you think about Linux is, which distribution to use? Given that I work for Red Hat, I have decided to use Fedora Workstation. This is mostly due to the Red Hat Enterprise Linux like behavior.\nUsability My impression is, that the usability has largely improved in the desktop space. The graphical user interface (GUI) looks very modern. You have the possibility to use multiple screens (as with MacOS). You can use the Software Center to obtain a ton of software including third parties, like rpmfusion and flatpaks. Most of the things can be configured via GUI, if you desire to do so.\nAre all problems solved now? Let me elaborate on the problems mentioned earlier.\nDrivers Per default Fedora was shipped with the NVIDIA nouveau drivers. Those drivers worked well for me on native Linux games. However, they performed very poorly on games which were relying on Wine. Yet again I had to install the GPU drivers from NVIDIA. To my surprise this was a very easy task. The software center had automatically detected which third party driver my system would need and I could install them very comfortably via the GUI. Gaming clients Unfortunately some gaming clients like the Blizzard Battle.net or GOG Galaxy do not offer native Linux support. Consequently you will need to take a look at other solutions. Currently there are differnet gaming clients available. The most common ones, besides Steam, are Heroic Games Launcher and Lutris. Both of those clients have their pros and cons. Lutris allows you to connect to the Battle.net but does not offer cloud save files for the GOG Galaxy. While the Heroic Games Launcher does support the cloud saves, you cannot use it for the Blizzard games. Consequently, there is no one size fits all solution.\nCloud saves An additional problem arises, when we talk about cloud saves. While, I cannot tell if this issue is persisnt across all gaming clients, but I can provide the following example: FTL: Advanced Edition is a strategy game which I had bought via GOG Galaxy (Note: I could have also bought it via Steam). To have my cloud save files, I would be required to use Heroic Games Launcher instead of Lutris. On the Heroic Games Launcher I have two options:\nInstall the Linux native version of the game (preferred) Install the Windows version of the game and use Wine. After trying option 1, I rather quickly found out that I do not have access to my cloud save files as they are tied to my previous operating system installation. After wiping the linux native installation, I had installed the Windows version. Then I had access to my save files. Funnily enough, the save files are just text files which you can copy around and thus use them even with the Linux version. Nevertheless, to get the access to the cloud save from the previous Windows installation, you will have to download the Windows version. Especially as the GOG Galaxy does not offer a download of the cloud saves neither via the website nor via an API.\nPerformance Before I can elaborate on the performance, I need to clarify, that there are no metrics being compared between the operating systems. I will just refer to the same games on equal graphic settings and see if those feel fluid (like before). The games I will be referring to are:\nHeroes of the Storm Frostpunk Elden Ring Heroes of the Storm Client: Lutris Game acquired via: GOG Galaxy Native platform(s): Windows Lutris allows you to install the Battle.net Desktop Client and then the game via the client as usual. This game works very well on Linux with equal graphic settings, i.e. High. When playing around with ALT+TAB and the Super/Windows key I found out that it is better to run this game with the Display Mode: Windowed Fullscreen. One thing which worked even better on Linux than Windows is, that I did not run into the Infinite Reconnecting screen which is some odd bug in the game which was never fixed.\nFrostpunk Client: Heroic Games Launcher Game acquired via: GOG Galaxy Native platform(s): Windows There was no issue with playing the game. It runs fluent and had no issues when tabbing in and out. The cloud saves caused some difficulties, but this seems to be game rather than platform specific.\nElden Ring Client: Steam Game acquired via: Steam Native platform(s): Windows This masterpiece is surely the most resource intense game on the list. Nevertheless, Linux was still able to run it without any issue on High graphic settings. Via Steam I could also easily connect my PS3 controller without any additional software and had access to all cloud saves.\nWhich games are problematic? Generally speaking games which use kernel-level anti-cheat technology might not run on linux due to the use of Wine or Proton. A list of games relying on kernel-level anti-cheat can be found here. If you look at the list very carefully, you will see that Elden Ring also uses kernel-level anti-cheat. This was no issue for me, as I was not playing online.\nI have a strong opinion that kernel-level anti-cheat gaming software should not be used, because the kernel is not a place for user software. Especially not gaming software. A human friendly one minute explanation can be found on Youtube from Pirate Software below: Conclusion The linux desktop has become way more user friendly than it was before. As you have seen, there might still be some minor issues, but it is not really blocking you from using it as a main operating system. My experience is very positive and I am more than happy that I had spent time on it.\n","date":"2025-01-31T00:00:00Z","image":"https://www.niederwanger.dev/p/gaming-on-linux-with-a-decade-old-hardware/gaming_hu_b8d592dfa6b1a6a0.jpg","permalink":"https://www.niederwanger.dev/p/gaming-on-linux-with-a-decade-old-hardware/","title":"Gaming on Linux with a decade old hardware"},{"content":"Building a blog is easy with the right tools As I was looking for a static site generator which supports Markdown as an input, I came across Hugo. Hugo is an open source static site generator written in Go. Hugo supports a lot of built in Themes. One easy Theme to get started is the Hugo Theme Stack from CaiJimmy.\n","date":"2025-01-15T00:00:00Z","image":"https://www.niederwanger.dev/p/building-a-blog/cover_hu_e95a4276bf860a84.jpg","permalink":"https://www.niederwanger.dev/p/building-a-blog/","title":"Building a blog with Hugo"}]